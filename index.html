<!DOCTYPE html>




<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Yuexiang Zhai's Home Page</title>
    <link rel="stylesheet" href="style.css">
</head>
<body>
    <div class="container">
        <h1><span class="hover">Yuexiang
            <span class="tooltip">Feel free to call me Simon.</span>
            </span> Zhai</h1>
        <section>
            <h2>Bio</h2>
            <p>
                I am a phd student at Berkeley EECS,
                advised by <a href="https://people.eecs.berkeley.edu/~yima/">Prof. Yi Ma</a>
                and <a href="https://people.eecs.berkeley.edu/~svlevine/">Prof. Sergey Levine</a>.
                I am also <span class="hover">affliated
                    <span class="tooltip">Because of my advisors, I have nothing to do with it
                        (except for having a seat at the beautiful Berkeley Way West office).</span>
                    </span> with <a href="https://bair.berkeley.edu/">BAIR</a>.
                I obtained an MS degree from Columbia University with <a href="https://www.columbia.edu/~jw2966/">Prof. John Wright</a>
                and a BS degree in <span class="hover">Math & Applied Math
                    <span class="tooltip">It is only one major,
                        I do not know why Zhejiang University (ZJU) created a major named
                        "Math & Applied Math".
                         <s>Perhaps ZJU is trying suggested that some math majors are not applicable?</s></span></span>
                         from <span class="hover">Zhejiang University
                    <span class="tooltip">Not from the Chu Kochen Honors College,
                         because I did pretty poorly during undergrad.</span></span>,

            </p>
        </section>
        <section>
            <h2>Contacts</h2>
            <ul>
                <li>Email: simonzhai at berkeley dot edu</li>
                <li><a href="https://twitter.com/simon_zhai">Twitter / X</a></li>
                <li><a href="https://github.com/YX-S-Z">Github</a></li>
                <li>Phone: <span class="hover">+1 (123) 5813 2134
                    <span class="tooltip">This is not my number, it is from the Fibonacci Sequence.
                         And please do not try to call it. </span></span></li>
            </ul>
        </section>
        <section>
            <h2>Research</h2>
            <ul>
                <li>Past experience: My past research spans different topics in machine learning,
                    reinforcement learning, and large models. </li>
                <li>Interest: I am interested in anything that
                    <span class="hover">I don't understand yet<span class="tooltip">
                    Such as (1) how does the universe works; (2) what are my cats talking. </span></span>. </li>
                <li>Current focus: I am currently focus on <span class="hover">things<span class="tooltip">
                    A.k.a. large multimodal models (Purely personal opinions,
                     feel free to debate me). </span></span>
                    that will work in the
                    <span class="hover">near future<span class="tooltip">
                        It means \leq 5 years. </span></span>, such as: </li>
                        <ul>
                            <li>training / applying multimodal models as decision-making agents </li>
                            <li>understanding the limitations of multimodal models </li>
                        </ul>
            </ul>

        </section>
        <section>
            <h2>Publications</h2>
            Please refer to my <a href="https://scholar.google.com/citations?user=78WTKm4AAAAJ&hl=en">Google Scholar</a>
            profile for my <span class="hover">full publication
                <span class="tooltip">Google scholar is a much better organizer than me</span></span> list.
                Some papers selected by topics are listed below. I shamelessly borrowed
                the style from <a href="https://people.csail.mit.edu/kaiming/">here</a>.

            <h3>Large Multimodal Models</h3>
            <div class="publication">
                <p class="title">Fine-Tuning Large Vision-Language Models as
                    Decision-Making Agents via Reinforcement Learning</p>
                <p><b>Yuexiang Zhai</b>, Hao Bai*, Zipeng Lin*, Jiayi Pan*, Shengbang Tong*, Yifei Zhou*,
                    Alane Suhr, Saining Xie, Yann LeCun, Yi Ma, Sergey Levine</p>
                <p>Tech report, May. 2024 </p>
                <p><a href="https://arxiv.org/abs/2405.10292">paper (arXiv)</a>
                    <a href="https://rl4vlm.github.io/">project</a>
                    <a href="https://github.com/RL4VLM/RL4VLM">code</a></p>
            </div>
            <div class="publication">
                <p class="title">Eyes Wide Shut? Exploring the Visual Shortcomings of Multimodal LLMs</p>
                <p>Shengbang Tong, Zhuang Liu, <b>Yuexiang Zhai</b>, Yi Ma, Yann LeCun, Saining Xie</p>
                <p>Conference on Computer Vision and Pattern Recognition (CVPR), 2024 (<b>Oral</b>). </p>
                <p><a href="https://arxiv.org/abs/2401.06209">paper (arXiv)</a>
                    <a href="https://tsb0601.github.io/mmvp_blog/#mmvp">project</a>
                    <a href="https://github.com/tsb0601/MMVP">code</a></p>
            </div>
            <div class="publication">
                <p class="title">Investigating the Catastrophic Forgetting in Multimodal Large Language Model Fine-Tuning</p>
                <p><b>Yuexiang Zhai</b>, Shengbang Tong, Xiao Li, Mu Cai, Qing Qu, Yong Jae Lee, Yi Ma</p>
                <p>Conference on Parsimony and Learning (CPAL), 2024. </p>
                <p><a href="https://proceedings.mlr.press/v234/zhai24a.html">paper</a>
                    <a href="https://yx-s-z.github.io/emt/">project</a></p>
            </div>
            <h3>Reinforcement Learning</h3>
            <div class="publication">
                <p class="title">Cal-QL: Calibrated Offline RL Pre-Training for Efficient Online Fine-Tuning</p>
                <p>Mitsuhiko Nakamoto*, <b>Yuexiang Zhai</b>*, Anikait Singh, Max Sobol Mark, Yi Ma, Chelsea Finn, Aviral Kumar, Sergey Levine</p>
                <p>Advances in Neural Information Processing Systems (NeurIPS), 2023. </p>
                <p><a href="https://www.jair.org/index.php/jair/article/view/13326">paper</a>
                    <a href="https://nakamotoo.github.io/Cal-QL/">project</a>
                    <a href="https://github.com/nakamotoo/Cal-QL">code</a></p>
            </div>
            <div class="publication">
                <p class="title">Understanding the Complexity Gains of Single-Task RL with a Curriculum</p>
                <p>Qiyang Li*, <b>Yuexiang Zhai</b>*, Yi Ma, Sergey Levine</p>
                <p>International Conference on Machine Learning (ICML), 2023.</p>
                <p><a href="https://proceedings.mlr.press/v202/li23as.html">paper</a></p>
            </div>
            <div class="publication">
                <p class="title">Computational Benefits of Intermediate Rewards for Goal-Reaching Policy Learning</p>
                <p><b>Yuexiang Zhai</b>, Christina Baek, Zhengyuan Zhou, Jiantao Jiao, Yi Ma</p>
                <p>Journal of Artificial Intelligence Research (JAIR), 2022.</p>
                <p><a href="https://www.jair.org/index.php/jair/article/view/13326">paper</a></p>
            </div>
            <h3>Machine Learning</h3>
            <div class="publication">
                <p class="title">Complete Dictionary Learning via L4-Norm Maximization over the Orthogonal Group</p>
                <p><b>Yuexiang Zhai</b>, Zitong Yang, Zhenyu Liao, John Wright, Yi Ma</p>
                <p>Journal of Machine Learning Research, 2020 (JMLR).</p>
                <p> Signal Processing with Adaptive Sparse Structured Representations 2019 (SPARS), <a class="red-text" href="http://spars-workshop.org/en/program/scientific-program.html">Best student paper finalist</a>.</p>
                <p><a href="https://www.jmlr.org/papers/v21/19-755.html">paper</a></p>
            </div>

            <div class="publication">
                <p class="title">Understanding L4-based Dictionary Learning: Interpretation, Stability, and Robustness</p>
                <p><b>Yuexiang Zhai</b>, Hermish Mehta, Zhengyuan Zhou, Yi Ma</p>
                <p>International Conference on Learning Representations (ICLR), 2020.</p>
                <p><a href="https://openreview.net/forum?id=SJeY-1BKDS">paper</a>
                    <a href="https://github.com/hermish/ZMZM-ICLR-2020">code</a></p>
            </div>
            <div class="publication">
                <p class="title">Geometric Analysis of Nonconvex Optimization Landscapes for Overcomplete Learning</p>
                <p>Qing Qu, <b>Yuexiang Zhai</b>, Xiao Li, Yuqian Zhang, Zhihui Zhu</p>
                <p>International Conference on Learning Representations (ICLR), 2020 (<b>Oral</b>).</p>
                <p><a href="https://openreview.net/forum?id=rygixkHKDH">paper</a></p>
            </div>
        </section>
    </div>
    <script src="script.js"></script>
</body>

</html>
